# -*- coding: utf-8 -*- 
# ------------------------------
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ------------------------------
import os 
import pandas as pd 
import torch 
import torch.nn as nn 
import torch.optim as optim 
from torch.utils.data import DataLoader, Dataset 
import numpy as np 
from PIL import Image, ImageDraw 
import random 
from torchvision import transforms 
import torch.nn.functional as F 
import time # íƒì§€ ì†ë„ ì¸¡ì •ì„ ìœ„í•´ time ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # ì˜¤ë¶„ë¥˜í‘œ ê³„ì‚° ë° ì‹œê°í™”ë¥¼ ìœ„í•´ ì¶”ê°€
import matplotlib.pyplot as plt # ì‹œê°í™”ë¥¼ ìœ„í•´ ì¶”ê°€

# ------------------------------
# ìƒìˆ˜ ì •ì˜ ë° ì‹¤ì‹œê°„ íƒì§€ ìµœì í™” ì„¤ì •
# ------------------------------
# ê²½ë¡œ (ì‹¤ì œ ë°ì´í„°ì…‹ì´ ì—†ëŠ” í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ë”ë¯¸ ê²½ë¡œ ì„¤ì •)
IMG_DIR = r"ê²½ë¡œ ì„¤ì •"
LABEL_CSV = r"ê²½ë¡œ ì„¤ì •"
TARGET_LABEL = 0 

# <<<< ì‹¤ì‹œê°„ íƒì§€ ì†ë„ ìµœì í™” ì„¤ì • >>>>
N_SAMPLES = 200 # íƒì§€ì— ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜ (í…ŒìŠ¤íŠ¸ í¬ê¸° ìœ ì§€)
N_PERTURBATIONS = 5 # ì´ë¯¸ì§€ë‹¹ ì„­ë™ íšŸìˆ˜ë¥¼ 10ì—ì„œ 5ë¡œ ì¶•ì†Œí•˜ì—¬ ì†ë„ ê°œì„ 
STRIP_THRESHOLD = 0.5 # ì—”íŠ¸ë¡œí”¼ ì„ê³„ê°’ (ë°±ë„ì–´: ì—”íŠ¸ë¡œí”¼ ë‚®ìŒ, í´ë¦°: ì—”íŠ¸ë¡œí”¼ ë†’ìŒ)
NUM_CLASSES = 43

# ------------------------------
# íŠ¸ë¦¬ê±°/ë°ì´í„°ì…‹/CNN ëª¨ë¸ í•¨ìˆ˜ 
# ------------------------------
def red_square(img):
    img = img.convert('RGB')
    np_img = np.array(img)
    h, w = np_img.shape[:2]
    np_img[h-5:h, w-5:w] = [255, 0, 0]
    return Image.fromarray(np_img)

def blue_circle(img):
    img = img.convert('RGB')
    draw = ImageDraw.Draw(img)
    draw.ellipse((5, 5, 15, 15), fill=(0, 0, 255))
    return img

def yellow_cross(img):
    img = img.convert('RGB')
    draw = ImageDraw.Draw(img)
    draw.line((0, 0, 15, 15), fill=(255, 255, 0), width=2)
    draw.line((15, 0, 0, 15), fill=(255, 255, 0), width=2)
    return img

def white_dots(img):
    img = img.convert('RGB')
    draw = ImageDraw.Draw(img)
    for _ in range(5):
        x, y = random.randint(0, 31), random.randint(0, 31)
        draw.point((x, y), fill=(255, 255, 255))
    return img

trigger_pool = [red_square, blue_circle, yellow_cross, white_dots]

class CustomGTSRB(Dataset):
    """GTSRB ë°ì´í„°ì…‹ì„ ëª¨ë°©í•˜ëŠ” í´ë˜ìŠ¤ (ì‹¤ì œ ê²½ë¡œê°€ ì—†ì„ ê²½ìš° ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©)"""
    def __init__(self, img_dir, label_csv, transform=None):
        self.img_dir = img_dir
        try:
            # --- [ì‹¤ì œ CSV ë¡œë”© ë¡œì§] ---
            df = pd.read_csv(label_csv, sep=';')
            df = df.dropna(subset=['ClassId'])
            df['ClassId'] = df['ClassId'].astype(int)
            self.labels = df.reset_index(drop=True)
            # ---------------------------
        except FileNotFoundError:
            # ë”ë¯¸ ë°ì´í„° ìƒì„± (íŒŒì¼ì´ ì—†ì„ ê²½ìš°)
            print("[ê²½ê³ ] CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            data = {'Filename': [f'{i:05d}.png' for i in range(100)], 
                    'ClassId': [i % NUM_CLASSES for i in range(100)]}
            self.labels = pd.DataFrame(data)
            
        self.transform = transform
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        row = self.labels.iloc[idx]
        img_path = os.path.join(self.img_dir, row['Filename'])
        label = row['ClassId']
        
        try:
            img = Image.open(img_path).convert('RGB')
        except FileNotFoundError:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            img = Image.new('RGB', (32, 32), color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))
            
        if self.transform:
            img = self.transform(img)
        return img, int(label)

class GTSRBBackdoor(Dataset):
    """ë°±ë„ì–´ ê³µê²©ì„ ìœ„í•œ ë°ì´í„°ì…‹ (íŠ¸ë¦¬ê±° ì‚½ì…)"""
    def __init__(self, dataset, trigger_ratio=0.5, target_label=0, transform=None):
        self.dataset = dataset
        self.trigger_ratio = trigger_ratio
        self.target_label = target_label
        self.transform = transform
    def __len__(self):
        return len(self.dataset)
    def __getitem__(self, idx):
        img, label = self.dataset[idx]
        if isinstance(img, torch.Tensor):
            img = transforms.ToPILImage()(img)
            
        if random.random() < self.trigger_ratio:
            img = random.choice(trigger_pool)(img)
            label = self.target_label
            
        if self.transform:
            img = self.transform(img)
        return img, int(label)

class TriggerOnlyDataset(Dataset):
    """ë°±ë„ì–´ ê³µê²© ì„±ê³µë¥ (ASR) ì¸¡ì •ì„ ìœ„í•œ ë°ì´í„°ì…‹"""
    def __init__(self, dataset, target_label=0, transform=None):
        self.dataset = dataset
        self.target_label = target_label
        self.transform = transform
    def __len__(self):
        return len(self.dataset)
    def __getitem__(self, idx):
        img, _ = self.dataset[idx]
        if isinstance(img, torch.Tensor):
            img = transforms.ToPILImage()(img)
            
        # í´ë¦° ì´ë¯¸ì§€ì— íŠ¸ë¦¬ê±°ë§Œ ì‚½ì…
        trigger_fn = random.choice(trigger_pool)
        img = trigger_fn(img)
        
        if self.transform:
            img = self.transform(img).float()
            
        label = torch.tensor(self.target_label).long()
        return img, label

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 128), nn.ReLU(),
            nn.Linear(128, num_classes)
        )
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# ------------------------------
# í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜
# ------------------------------
def train(model, loader, optimizer, criterion, device):
    """ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë‹¨ì¼ ì—í¬í¬ ì‹¤í–‰ í•¨ìˆ˜"""
    model.train()
    for imgs, labels in loader:
        imgs = imgs.to(device)
        labels = labels.to(device).long()
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

def evaluate(model, loader, device):
    """ëª¨ë¸ í‰ê°€ (ì •í™•ë„ ê³„ì‚°) í•¨ìˆ˜"""
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs = imgs.to(device)
            labels = labels.to(device).long()
            outputs = model(imgs)
            preds = outputs.argmax(1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return 100.0 * correct / total

# ------------------------------
# STRIP ë°©ì–´ ê¸°ë²• í•µì‹¬ í•¨ìˆ˜ (ì‹¤ì‹œê°„ ìµœì í™” ì ìš©)
# ------------------------------
def compute_entropy(probs):
    """ì£¼ì–´ì§„ í™•ë¥  ë¶„í¬ì— ëŒ€í•œ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    # log(prob + epsilon)ì„ ì‚¬ìš©í•˜ì—¬ log(0) ë°©ì§€
    log_probs = F.log_softmax(torch.log(probs + 1e-10), dim=1) 
    return -torch.sum(probs * log_probs, dim=1)

def strip_detection(model, dataset, device, n_perturbations=N_PERTURBATIONS, n_samples=N_SAMPLES):
    """
    STRIP íƒì§€ í•¨ìˆ˜ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”).
    """
    model.eval()
    entropies = []
    
    max_samples = min(n_samples, len(dataset))
    
    # ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ì‚¬ì „ ë¡œë“œ (ëœë¤ ì ‘ê·¼ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
    # ë°ì´í„°ì…‹ì´ í° ê²½ìš°, ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” N_SAMPLESê°€ ì‘ìœ¼ë¯€ë¡œ ì„ì‹œë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    try:
        noise_images = [dataset[i][0].to(device) for i in range(len(dataset))]
    except Exception as e:
        print(f"[ê²½ê³ ] ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë”ë¯¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        dummy_img = torch.rand(3, 32, 32).to(device)
        noise_images = [dummy_img] * max_samples


    start_time = time.time() # íƒì§€ ì‹œì‘ ì‹œê°„ ì¸¡ì •
    
    with torch.no_grad():
        for i in range(max_samples):
            try:
                img, _ = dataset[i]
            except IndexError:
                # ë”ë¯¸ ë°ì´í„°ì…‹ í¬ê¸°ê°€ N_SAMPLESë³´ë‹¤ ì‘ì„ ê²½ìš° ì²˜ë¦¬
                img, _ = dataset[random.randint(0, len(dataset)-1)] 
                
            img = img.unsqueeze(0).to(device) # ì›ë³¸ ì´ë¯¸ì§€ (1x3x32x32)
            
            perturbed_batch = [] 

            for _ in range(n_perturbations):
                # ëœë¤ ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ ì„ íƒ
                noise_img = random.choice(noise_images)
                
                # ë¯¹ì‹± ì²˜ë¦¬: M = (I + N) / 2
                mixed = (img.squeeze(0) + noise_img) / 2.0
                
                perturbed_batch.append(mixed.unsqueeze(0))

            # ì„­ë™ëœ ì´ë¯¸ì§€ë“¤ì„ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë¬¶ì–´ GPUì—ì„œ ë³‘ë ¬ ì²˜ë¦¬
            inputs = torch.cat(perturbed_batch, dim=0)
            
            # ìˆœì „íŒŒ
            outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)
            
            # í‰ê·  ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            entropy = compute_entropy(probs).mean().item()
            entropies.append(entropy)

    end_time = time.time() # íƒì§€ ì¢…ë£Œ ì‹œê°„ ì¸¡ì •
    detection_time = end_time - start_time
    
    return entropies, detection_time

def calculate_detection_metrics(clean_entropies, backdoor_entropies, threshold=STRIP_THRESHOLD):
    """
    STRIP íƒì§€ ì„±ëŠ¥ ì§€í‘œ (2x2 ì˜¤ë¶„ë¥˜í‘œ)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    (íƒì§€ ê¸°ì¤€: ì—”íŠ¸ë¡œí”¼ < ì„ê³„ê°’)
    """
    # ë°±ë„ì–´ ìƒ˜í”Œ (Actual Backdoor): ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì•„ì•¼ íƒì§€ë¨
    backdoor_detected = sum([e < threshold for e in backdoor_entropies]) # TP (íƒì§€ ì„±ê³µ)
    backdoor_not_detected = sum([e >= threshold for e in backdoor_entropies]) # FN (ë¯¸íƒì§€)
    
    # í´ë¦° ìƒ˜í”Œ (Actual Clean): ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì•„ì•¼ ê±°ë¶€ë¨
    clean_detected = sum([e < threshold for e in clean_entropies]) # FP (ì˜¤íƒì§€)
    clean_not_detected = sum([e >= threshold for e in clean_entropies]) # TN (ì˜¬ë°”ë¥¸ ê±°ë¶€)
    
    TP = backdoor_detected
    FN = backdoor_not_detected
    FP = clean_detected
    TN = clean_not_detected
    
    total_samples = TP + FN + FP + TN

    TPR = TP / (TP + FN) * 100 if (TP + FN) > 0 else 0.0 # ë°±ë„ì–´ íƒì§€ìœ¨ (Detection Accuracy)
    FPR = FP / (FP + TN) * 100 if (FP + TN) > 0 else 0.0 # ì˜¤íƒì§€ìœ¨ (False Positive Rate)
    
    return TP, FN, FP, TN, TPR, FPR, total_samples

# ------------------------------
# ë©”ì¸ ì‹¤í–‰ ë° STRIP íƒì§€ ìˆ˜í–‰
# ------------------------------
transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor()
])

# ë°ì´í„°ì…‹ ë¡œë”©
try:
    base_dataset = CustomGTSRB(IMG_DIR, LABEL_CSV, transform=None)
    # í•™ìŠµ ë°ì´í„°ì…‹ (10% ì˜¤ì—¼)
    train_dataset = GTSRBBackdoor(base_dataset, trigger_ratio=0.1, target_label=TARGET_LABEL, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) 
    # íƒì§€ ë°ì´í„°ì…‹
    test_clean_dataset = CustomGTSRB(IMG_DIR, LABEL_CSV, transform=transform)
    test_backdoor_dataset = TriggerOnlyDataset(test_clean_dataset, target_label=TARGET_LABEL, transform=transform)
except Exception as e:
    print(f"\n[ì˜¤ë¥˜ ë°œìƒ] ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    print("GTSRB ë°ì´í„°ì…‹ ê²½ë¡œ(IMG_DIR, LABEL_CSV)ë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë”ë¯¸ ë°ì´í„° ì‚¬ìš©ì— ë¬¸ì œê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ì‹­ì‹œì˜¤.")
    exit()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleCNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# ëª¨ë¸ í•™ìŠµ (STRIP íƒì§€ ì „ ë‹¨ê³„)
print("\n[ë°±ë„ì–´ ëª¨ë¸ í•™ìŠµ ì¤‘...]")
for epoch in range(5):
    train(model, train_loader, optimizer, criterion, device)

# ------------------------------
# STRIP íƒì§€ ìˆ˜í–‰ ë° ì¸¡ì •
# ------------------------------
print(f"\n[STRIP íƒì§€ ìˆ˜í–‰ ì¤‘ - {N_SAMPLES*2}ê°œ ìƒ˜í”Œ, {N_PERTURBATIONS} ì„­ë™]")

# 1. í´ë¦° ìƒ˜í”Œ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ë° ì‹œê°„ ì¸¡ì •
clean_entropies, clean_time = strip_detection(
    model, test_clean_dataset, device, 
    n_perturbations=N_PERTURBATIONS, n_samples=N_SAMPLES
)

# 2. ë°±ë„ì–´ ìƒ˜í”Œ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ë° ì‹œê°„ ì¸¡ì •
backdoor_entropies, backdoor_time = strip_detection(
    model, test_backdoor_dataset, device, 
    n_perturbations=N_PERTURBATIONS, n_samples=N_SAMPLES
)

# ì´ íƒì§€ ì†ë„
total_detection_time = clean_time + backdoor_time
total_test_samples = N_SAMPLES * 2

# ìƒ˜í”Œë‹¹ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
time_per_sample = total_detection_time / total_test_samples if total_test_samples > 0 else 0.0

# 3. íƒì§€ ì„±ëŠ¥ ì§€í‘œ (2x2 ì˜¤ë¶„ë¥˜í‘œ) ê³„ì‚°
TP, FN, FP, TN, TPR, FPR, total_used_samples = calculate_detection_metrics(
    clean_entropies, backdoor_entropies, threshold=STRIP_THRESHOLD
)

# --------------------------------------------------------------------------
# ğŸŒŸ í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ì½”ë“œ (2x2 ë§¤íŠ¸ë¦­ìŠ¤: Backdoor vs Clean) ğŸŒŸ
# --------------------------------------------------------------------------

# 2x2 Confusion Matrix êµ¬ì„± (Actual vs Predicted)
# Predicted: Backdoor (Detected), Predicted: Clean (Rejected)
cm_2x2 = np.array([
    [TP, FN], # Actual Backdoor
    [FP, TN]  # Actual Clean
])

# ì‹œê°í™” (ì •ìˆ˜ countë¡œ í‘œì‹œ)
disp = ConfusionMatrixDisplay(
    confusion_matrix=cm_2x2, 
    display_labels=['Predicted: Backdoor (Detected)', 'Predicted: Clean (Rejected)']
)
disp.plot(cmap='Blues', values_format='d') 

plt.title(f"STRIP Detection 2x2 Confusion Matrix (Threshold: {STRIP_THRESHOLD})")
plt.xlabel("Predicted Label (STRIP Output)")
plt.ylabel("Actual Sample Type")

# y-ì¶• ë ˆì´ë¸”ì„ ìˆ˜ë™ìœ¼ë¡œ 'Actual Backdoor'ì™€ 'Actual Clean'ìœ¼ë¡œ ì„¤ì •
ax = plt.gca()
ax.set_yticklabels(['Actual Backdoor', 'Actual Clean'])
plt.show()

# --------------------------------------------------------------------------


# 4. ê²°ê³¼ ì¶œë ¥ (ì˜¤ë¶„ë¥˜í‘œ í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •)
print("\n" + "="*70)
print("                       STRIP ì‹¤ì‹œê°„ íƒì§€ ë¶„ì„ ")
print("                      (ì—”íŠ¸ë¡œí”¼ ì„ê³„ê°’: %.2f)" % STRIP_THRESHOLD)
print("-" * 70)
print("Actual |    Predicted: Backdoor (Detected) |  Predicted: Clean (Rejected) |")
print("-" * 70)
print(f"Backdoor|    Â  Â  Â  Â {TP:6d} (TP) Â  Â  Â  Â  Â |    Â  Â  Â  Â {FN:6d} (FN) Â  Â  Â  Â  |")
print("-" * 70)
print(f"Clean |    Â  Â  Â  Â {FP:6d} (FP) Â  Â  Â  Â  Â |    Â  Â  Â  Â {TN:6d} (TN) Â  Â  Â  Â  |")
print("-" * 70)
print(f"\n[ë°±ë„ì–´ íƒì§€ ì •í™•ë„ (TPR)]: Â  Â {TPR:.2f}% (TP / Actual Backdoor)")
print(f"[í´ë¦° ì˜¤íƒì§€ìœ¨ (FPR)]: Â  Â  Â  Â {FPR:.2f}% (FP / Actual Clean)")
print(f"[ì´ íƒì§€ ì†Œìš” ì‹œê°„]: Â  Â  Â  Â  Â {total_detection_time:.4f} ì´ˆ ({total_used_samples} ìƒ˜í”Œ ê¸°ì¤€)")
print(f"[ìƒ˜í”Œë‹¹ í‰ê·  ì²˜ë¦¬ ì†ë„]: Â  Â  Â {time_per_sample * 1000:.2f} ms/ìƒ˜í”Œ")
print("="*70)

